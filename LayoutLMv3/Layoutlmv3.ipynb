{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEloAibG4MKH",
        "outputId": "4d046945-9eea-4fd8-aded-a0ec487dcf4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Dec 11 03:42:50 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z4IFmdXiVEZ",
        "outputId": "12ff4dd9-1355-42b6-a4d2-8bcfc7fd4b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfkit in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "Note, selecting 'wkhtmltox' instead of './wkhtmltopdf.deb'\n",
            "Some packages could not be installed. This may mean that you have\n",
            "requested an impossible situation or if you are using the unstable\n",
            "distribution that some required packages have not yet been created\n",
            "or been moved out of Incoming.\n",
            "The following information may help to resolve the situation:\n",
            "\n",
            "The following packages have unmet dependencies:\n",
            " wkhtmltox : Depends: libssl1.1 but it is not installable\n",
            "E: Unable to correct problems, you have held broken packages.\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfkit\n",
        "!wget -q https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb -O wkhtmltopdf.deb\n",
        "!sudo apt-get install -y ./wkhtmltopdf.deb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxYQv_BeoS0U"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq transformers==4.27.2 --progress-bar off\n",
        "!pip install -qqq pytorch-lightning==1.9.4 --progress-bar off\n",
        "!pip install -qqq torchmetrics==0.11.4 --progress-bar off\n",
        "!pip install -qqq imgkit==1.2.3 --progress-bar off\n",
        "!pip install -qqq easyocr==1.6.2 --progress-bar off\n",
        "!pip install -qqq Pillow==9.4.0 --progress-bar off\n",
        "!pip install -qqq tensorboardX==2.5.1 --progress-bar off\n",
        "!pip install -qqq huggingface_hub==0.11.1 --progress-bar off\n",
        "!pip install -qqq --upgrade --no-cache-dir gdown\n",
        "!pip install --upgrade huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyZi7kODojA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fd08172-7720-4ead-dd5b-5fd974933e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from transformers import LayoutLMv3FeatureExtractor, LayoutLMv3TokenizerFast, LayoutLMv3Processor, LayoutLMv3ForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import imgkit\n",
        "import easyocr\n",
        "import torchvision.transforms as T\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from typing import List\n",
        "import json\n",
        "from torchmetrics import Accuracy\n",
        "from huggingface_hub import notebook_login\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "%matplotlib inline\n",
        "pl.seed_everything(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y43stImRClfq",
        "outputId": "c86e7b08-dc41-4be4-f168-645cd4b300c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1xa2SDBjrYpBKrloKp4Yj9_cAEUwU7LHz\n",
            "From (redirected): https://drive.google.com/uc?id=1xa2SDBjrYpBKrloKp4Yj9_cAEUwU7LHz&confirm=t&uuid=82acf285-9dfb-4385-a44b-38509ce218ff\n",
            "To: /content/unstructured-Documents-Maana.zip\n",
            "100% 184M/184M [00:05<00:00, 34.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1xa2SDBjrYpBKrloKp4Yj9_cAEUwU7LHz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhDzIzpeCptP"
      },
      "outputs": [],
      "source": [
        "!unzip -q unstructured-Documents-Maana.zip\n",
        "!mv \"unstructured-berkeley-project-1-documents/\" \"documents\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkJxI9iyv0Ff"
      },
      "source": [
        "## Convert HTML to images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WnVAitHd3XJ"
      },
      "outputs": [],
      "source": [
        "for dir in Path(\"documents\").glob(\"*\"):\n",
        "    dir.rename(str(dir).lower().replace(\" \", \"_\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuk-fvPxeWPd"
      },
      "outputs": [],
      "source": [
        "list(Path(\"documents\").glob(\"*\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuNrOChau0Do"
      },
      "outputs": [],
      "source": [
        "for dir in Path(\"documents\").glob(\"*\"):\n",
        "    image_dir = Path(f\"images/{dir.name}\")\n",
        "    image_dir.mkdir(exist_ok=True, parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSHPf4mXWL1p"
      },
      "outputs": [],
      "source": [
        "def convert_html_to_image(file_path: Path, images_dir: Path, scale: float = 1.0) -> Path:\n",
        "    file_name = file_path.with_suffix(\".jpg\").name\n",
        "    save_path = images_dir / file_path.parent.name / f\"{file_name}\"\n",
        "    imgkit.from_file(str(file_path), save_path, options={'quiet': '', 'format': 'jpeg'})\n",
        "\n",
        "    image = Image.open(save_path)\n",
        "    width, height = image.size\n",
        "    image = image.resize((int(width * scale), int(height * scale)))\n",
        "    image.save(str(save_path))\n",
        "\n",
        "    return save_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO_nhR5dSU5P"
      },
      "outputs": [],
      "source": [
        "document_paths = list(Path(\"documents\").glob(\"*/*\"))\n",
        "\n",
        "for doc_path in tqdm(document_paths):\n",
        "    convert_html_to_image(doc_path, Path(\"images\"), scale=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICALpNlwfCOv"
      },
      "outputs": [],
      "source": [
        "image_paths = sorted(list(Path(\"images\").glob(\"*/*.jpg\")))\n",
        "\n",
        "image = Image.open(image_paths[0]).convert(\"RGB\")\n",
        "width, height = image.size\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHm3Sz5oICCH"
      },
      "source": [
        "## EasyOCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2QHnXiAIRUi"
      },
      "outputs": [],
      "source": [
        "reader = easyocr.Reader(['en'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WMFFiaDn8-o"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "image_path = image_paths[0]\n",
        "ocr_result = reader.readtext(str(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ocr_result[0]"
      ],
      "metadata": {
        "id": "YzlNIyHijS6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBdUxRMvAcFA"
      },
      "outputs": [],
      "source": [
        "font_path = Path(cv2.__path__[0]) / \"qt/fonts/DejaVuSansCondensed.ttf\"\n",
        "print(font_path.exists())\n",
        "font = ImageFont.truetype(str(font_path), size=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvI7GpqeCEo3"
      },
      "outputs": [],
      "source": [
        "def create_bounding_box(bbox_data):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for x, y in bbox_data:\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "\n",
        "    left = int(min(xs))\n",
        "    top = int(min(ys))\n",
        "    right = int(max(xs))\n",
        "    bottom = int(max(ys))\n",
        "\n",
        "    return [left, top, right, bottom]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klRKJdxSoATL"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(28, 28))\n",
        "\n",
        "left_image = Image.open(image_path).convert(\"RGB\")\n",
        "right_image = Image.new(\"RGB\", left_image.size, (255, 255, 255))\n",
        "\n",
        "left_draw = ImageDraw.Draw(left_image)\n",
        "right_draw = ImageDraw.Draw(right_image)\n",
        "\n",
        "for i, (bbox, word, confidence) in enumerate(ocr_result):\n",
        "    box = create_bounding_box(bbox)\n",
        "\n",
        "    left_draw.rectangle(box, outline=\"blue\", width=2)\n",
        "    left, top, right, bottom = box\n",
        "\n",
        "    left_draw.text((right + 5, top), text=str(i + 1), fill=\"red\", font=font)\n",
        "    right_draw.text((left, top), text=word, fill=\"black\", font=font)\n",
        "\n",
        "ax1.imshow(left_image)\n",
        "ax2.imshow(right_image)\n",
        "ax1.axis(\"off\");\n",
        "ax2.axis(\"off\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O7D7_L2i7NI"
      },
      "outputs": [],
      "source": [
        "for image_path in tqdm(image_paths):\n",
        "    ocr_result = reader.readtext(str(image_path), batch_size=16)\n",
        "\n",
        "    ocr_page = []\n",
        "    for bbox, word, confidence in ocr_result:\n",
        "        ocr_page.append({\n",
        "            \"word\": word, \"bounding_box\": create_bounding_box(bbox)\n",
        "        })\n",
        "\n",
        "    with image_path.with_suffix(\".json\").open(\"w\") as f:\n",
        "        json.dump(ocr_page, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDv-aCtFrCJS"
      },
      "outputs": [],
      "source": [
        "!tar -cJf financial-documents-ocr.tar.xz \"./images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pF-rQhx_7eqN"
      },
      "outputs": [],
      "source": [
        "!gdown 1bQ4mFbVRUtOEJSe8b4hUYIcngSgfdldw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSd8zXa37uln"
      },
      "outputs": [],
      "source": [
        "!tar -xf financial-documents-ocr.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths = sorted(list(Path(\"images\").glob(\"*/*.jpg\")))"
      ],
      "metadata": {
        "id": "voxz4OjfY5KD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj-R6x7Vv3L1"
      },
      "source": [
        "## LayoutLMv3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0F9tPyVxkza"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d68RauHOOeXo"
      },
      "outputs": [],
      "source": [
        "feature_extractor = LayoutLMv3FeatureExtractor(apply_ocr=False)\n",
        "tokenizer = LayoutLMv3TokenizerFast.from_pretrained(\"microsoft/layoutlmv3-base\")\n",
        "processor = LayoutLMv3Processor(feature_extractor, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oi4aU36W7J-1"
      },
      "source": [
        "Calling the processor is similar to a using a tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUOEyCRZeMft"
      },
      "outputs": [],
      "source": [
        "image_path = image_paths[0]\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "width, height = image.size\n",
        "width, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yG1YxFe1eR9V"
      },
      "outputs": [],
      "source": [
        "width_scale = 1000 / width\n",
        "height_scale = 1000 / height\n",
        "\n",
        "width_scale, height_scale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yxnz_jp-vnB"
      },
      "outputs": [],
      "source": [
        "json_path = image_path.with_suffix(\".json\")\n",
        "json_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjH4kl18_GIm"
      },
      "outputs": [],
      "source": [
        "with json_path.open(\"r\") as f:\n",
        "    ocr_result = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_bounding_box(box: List[int], width_scale : float = 1.0, height_scale : float = 1.0) -> List[int]:\n",
        "    return [\n",
        "        int(box[0] * width_scale),\n",
        "        int(box[1] * height_scale),\n",
        "        int(box[2] * width_scale),\n",
        "        int(box[3] * height_scale)\n",
        "    ]"
      ],
      "metadata": {
        "id": "vpF65O79zhVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBXx6khEdoa8"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "boxes = []\n",
        "for row in ocr_result:\n",
        "    boxes.append(scale_bounding_box(row[\"bounding_box\"], width_scale, height_scale))\n",
        "    words.append(row[\"word\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7HmNccDg_Kl"
      },
      "outputs": [],
      "source": [
        "len(words), len(boxes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcSZs_pePegQ"
      },
      "outputs": [],
      "source": [
        "encoding = processor(\n",
        "    image,\n",
        "    words,\n",
        "    boxes=boxes,\n",
        "    max_length=512,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-xwHVciBNNG"
      },
      "outputs": [],
      "source": [
        "print(f\"\"\"\n",
        "input_ids:  {list(encoding[\"input_ids\"].squeeze().shape)}\n",
        "word boxes: {list(encoding[\"bbox\"].squeeze().shape)}\n",
        "image data: {list(encoding[\"pixel_values\"].squeeze().shape)}\n",
        "image size: {image.size}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJtIgej0GK_V"
      },
      "source": [
        "##### Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bf7G4Iww46yP"
      },
      "outputs": [],
      "source": [
        "image_data = encoding[\"pixel_values\"][0]\n",
        "image_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3I4UKRorTXx"
      },
      "outputs": [],
      "source": [
        "transform = T.ToPILImage()\n",
        "transform(image_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKD5qsSVIExY"
      },
      "source": [
        "Word boxes use the `[left, top, right, bottom]` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JPqNlwWVmP6"
      },
      "outputs": [],
      "source": [
        "def unnormalize_box(bbox, width, height):\n",
        "    return [\n",
        "        width * (bbox[0] / 1000),\n",
        "        height * (bbox[1] / 1000),\n",
        "        width * (bbox[2] / 1000),\n",
        "        height * (bbox[3] / 1000),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJzjCh3_KKgX"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mReG4eztPqdK"
      },
      "outputs": [],
      "source": [
        "model = LayoutLMv3ForSequenceClassification.from_pretrained(\"microsoft/layoutlmv3-base\", num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrXLqq1XKbO8"
      },
      "outputs": [],
      "source": [
        "model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kq2eMkDjPf9r"
      },
      "outputs": [],
      "source": [
        "encoding = processor(\n",
        "    image,\n",
        "    words,\n",
        "    boxes=boxes,\n",
        "    max_length=512,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "outputs = model(**encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GFid5oDQ3eI"
      },
      "outputs": [],
      "source": [
        "outputs.logits"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "k_mhcztivaTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import Levenshtein\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_tagged_string(input_string):\n",
        "    \"\"\"\n",
        "    Parses a string with tags like <page_header>, <paragraph>, <sep/> into a structured dictionary.\n",
        "    Handles unclosed or malformed tags.\n",
        "\n",
        "    Args:\n",
        "        input_string (str): The input string.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary representing the structured data, or None if parsing fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Remove <s>, </s>, and <sep/> tags\n",
        "        input_string = (\n",
        "            input_string.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<sep/>\", \"\")\n",
        "        )\n",
        "        tag_positions = []\n",
        "        for match in re.finditer(r\"<(/?)(\\w+)>\", input_string):\n",
        "            tag_positions.append(\n",
        "                (match.start(), match.group(1) == \"/\", match.group(2))\n",
        "            )\n",
        "\n",
        "        elements = []\n",
        "        start = 0\n",
        "        current_tag = None\n",
        "\n",
        "        for pos, is_closing, tag_name in tag_positions:\n",
        "            if not is_closing:\n",
        "                #  Handles the case where Previous tag was not closed\n",
        "                if current_tag is not None:\n",
        "                    content = input_string[start:pos].strip()\n",
        "                    if content and current_tag in [\n",
        "                        \"page_header\",\n",
        "                        \"paragraph\",\n",
        "                        \"subheading\",\n",
        "                        \"title\",\n",
        "                        \"image\",\n",
        "                        \"table\",\n",
        "                        \"code_snippet\",\n",
        "                        \"page_footer\",\n",
        "                    ]:\n",
        "                        elements.append({\"type\": current_tag, \"content\": content})\n",
        "\n",
        "                current_tag = tag_name\n",
        "                start = pos + len(f\"<{tag_name}>\")\n",
        "            else:\n",
        "                if current_tag == tag_name:\n",
        "                    content = input_string[start:pos].strip()\n",
        "                    if content and current_tag in [\n",
        "                        \"page_header\",\n",
        "                        \"paragraph\",\n",
        "                        \"subheading\",\n",
        "                        \"title\",\n",
        "                        \"image\",\n",
        "                        \"table\",\n",
        "                        \"code_snippet\",\n",
        "                        \"page_footer\",\n",
        "                    ]:\n",
        "                        elements.append({\"type\": current_tag, \"content\": content})\n",
        "\n",
        "                    current_tag = None\n",
        "                    start = pos + len(f\"</{tag_name}>\")\n",
        "\n",
        "        if current_tag is not None and start < len(input_string):\n",
        "            content = input_string[start:].strip()\n",
        "            if content and current_tag in [\n",
        "                \"page_header\",\n",
        "                \"paragraph\",\n",
        "                \"subheading\",\n",
        "                \"title\",\n",
        "                \"image\",\n",
        "                \"table\",\n",
        "                \"code_snippet\",\n",
        "                \"page_footer\",\n",
        "            ]:\n",
        "                elements.append({\"type\": current_tag, \"content\": content})\n",
        "\n",
        "        return {\"document\": elements}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during parsing: {e}\")\n",
        "        return None\n",
        "\n",
        "def safe_divide(numerator, denominator):\n",
        "    \"\"\"Performs safe division, handling potential division by zero.\"\"\"\n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator / denominator\n",
        "\n",
        "def calculate_text_accuracy(ref_text, pred_text):\n",
        "    \"\"\"\n",
        "    Calculates text accuracy using Levenshtein distance.\n",
        "\n",
        "    Args:\n",
        "        ref_text (str): Reference text.\n",
        "        pred_text (str): Predicted text.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (accuracy, word_count) where accuracy is a float between 0 and 1,\n",
        "               and word_count is the number of words in the reference text.\n",
        "    \"\"\"\n",
        "    if not ref_text:\n",
        "        if not pred_text:\n",
        "            return 1.0, 0  # Both empty is considered 100% accurate\n",
        "        else:\n",
        "            return 0.0, 0\n",
        "    if not isinstance(ref_text, str) or not isinstance(pred_text, str):\n",
        "        return 0.0, 0\n",
        "    dist = Levenshtein.distance(ref_text, pred_text)\n",
        "    ref_word_count = len(ref_text.split())\n",
        "    accuracy = safe_divide(max(0, len(ref_text) - dist), len(ref_text))\n",
        "\n",
        "    return accuracy, ref_word_count\n",
        "\n",
        "def calculate_overall_text_extraction_accuracy(reference, prediction):\n",
        "    total_accuracy = 0\n",
        "    total_words = 0\n",
        "\n",
        "    ref_texts = [item[\"content\"] for item in reference[\"document\"]]\n",
        "    pred_texts = [item[\"content\"] for item in prediction[\"document\"]]\n",
        "\n",
        "    for ref_text, pred_text in zip(ref_texts, pred_texts):\n",
        "        accuracy, word_count = calculate_text_accuracy(ref_text, pred_text)\n",
        "        total_accuracy += accuracy * word_count\n",
        "        total_words += word_count\n",
        "\n",
        "    for pred_text in pred_texts[len(ref_texts):]:\n",
        "        accuracy, word_count = calculate_text_accuracy(\"\", pred_text)\n",
        "        total_accuracy += accuracy * word_count\n",
        "        total_words += word_count\n",
        "\n",
        "    overall_accuracy = safe_divide(total_accuracy, total_words)\n",
        "\n",
        "    return overall_accuracy\n",
        "\n",
        "def calculate_tag_categorization_accuracy(reference, prediction):\n",
        "    \"\"\"\n",
        "    Calculates tag categorization accuracy and F1 score.\n",
        "\n",
        "    Args:\n",
        "        reference (dict): Parsed reference document.\n",
        "        prediction (dict): Parsed prediction document.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (tag_accuracy_dict, overall_accuracy, tag_f1_dict, overall_f1)\n",
        "               where tag_accuracy_dict is accuracy per tag, overall_accuracy is overall tag accuracy,\n",
        "               tag_f1_dict is F1 score per tag, and overall_f1 is overall F1 score.\n",
        "    \"\"\"\n",
        "    ref_tags = [item[\"type\"] for item in reference[\"document\"]]\n",
        "    pred_tags = [item[\"type\"] for item in prediction[\"document\"]]\n",
        "\n",
        "    # Predefined list of tags\n",
        "    all_tags = [\n",
        "        \"page_header\",\n",
        "        \"paragraph\",\n",
        "        \"subheading\",\n",
        "        \"title\",\n",
        "        \"image\",\n",
        "        \"table\",\n",
        "        \"code_snippet\",\n",
        "        \"page_footer\",\n",
        "    ]\n",
        "\n",
        "    tag_accuracy_dict = {}\n",
        "    tag_f1_dict = {}\n",
        "    overall_tp = 0\n",
        "    overall_fp = 0\n",
        "    overall_fn = 0\n",
        "\n",
        "    for tag in all_tags:\n",
        "        tp = 0\n",
        "        fp = 0\n",
        "        fn = 0\n",
        "\n",
        "        for r_tag, p_tag in zip(ref_tags, pred_tags):\n",
        "            if r_tag == tag:\n",
        "                if p_tag == tag:\n",
        "                    tp += 1 # True Positive\n",
        "                else:\n",
        "                    fn += 1 # False Negative\n",
        "            elif p_tag == tag:\n",
        "                fp += 1     # False Positive\n",
        "\n",
        "        overall_tp += tp\n",
        "        overall_fp += fp\n",
        "        overall_fn += fn\n",
        "\n",
        "        precision = safe_divide(tp, tp + fp)\n",
        "        recall = safe_divide(tp, tp + fn)\n",
        "        f1_score = safe_divide(2 * precision * recall, precision + recall)\n",
        "\n",
        "        correct_for_tag = tp  # Correct for tag is TP in this context\n",
        "        total_for_tag_ref = ref_tags.count(tag)\n",
        "\n",
        "        if total_for_tag_ref > 0:\n",
        "            tag_accuracy_dict[tag] = safe_divide(correct_for_tag, total_for_tag_ref)\n",
        "        else:\n",
        "            tag_accuracy_dict[tag] = -9999  # Use -9999 for tags not present in reference\n",
        "\n",
        "        tag_f1_dict[tag] = f1_score if total_for_tag_ref > 0 or tag in pred_tags else -9999 # F1 score -9999 if tag not in ref and pred\n",
        "\n",
        "\n",
        "    correct_tags = overall_tp # Overall correct tags is sum of TPs\n",
        "    overall_accuracy = safe_divide(\n",
        "        correct_tags, max(len(ref_tags), len(pred_tags))\n",
        "    )\n",
        "\n",
        "    overall_precision = safe_divide(overall_tp, overall_tp + overall_fp)\n",
        "    overall_recall = safe_divide(overall_tp, overall_tp + overall_fn)\n",
        "    overall_f1 = safe_divide(2 * overall_precision * overall_recall, overall_precision + overall_recall)\n",
        "\n",
        "    return tag_accuracy_dict, overall_accuracy, tag_f1_dict, overall_f1\n",
        "\n",
        "def calculate_text_accuracy_for_tag(reference, prediction, tag):\n",
        "    \"\"\"\n",
        "    Calculates text extraction accuracy for a specific tag, handling missing tags.\n",
        "    \"\"\"\n",
        "    total_accuracy = 0\n",
        "    total_words = 0\n",
        "\n",
        "    ref_texts = [item[\"content\"] for item in reference[\"document\"] if item[\"type\"] == tag]\n",
        "    pred_texts = [item[\"content\"] for item in prediction[\"document\"] if item[\"type\"] == tag]\n",
        "\n",
        "    # If the tag is missing in both reference and prediction, return -9999 because it's not evaluated\n",
        "    if not ref_texts and not pred_texts:\n",
        "        return -9999\n",
        "\n",
        "    for ref_text, pred_text in zip(ref_texts, pred_texts):\n",
        "        accuracy, word_count = calculate_text_accuracy(ref_text, pred_text)\n",
        "        total_accuracy += accuracy * word_count\n",
        "        total_words += word_count\n",
        "\n",
        "    for ref_text in ref_texts[len(pred_texts):]:\n",
        "        accuracy, word_count = calculate_text_accuracy(ref_text, \"\")\n",
        "        total_accuracy += accuracy * word_count\n",
        "        total_words += word_count\n",
        "\n",
        "    for pred_text in pred_texts[len(ref_texts):]:\n",
        "        accuracy, word_count = calculate_text_accuracy(\"\", pred_text)\n",
        "        total_accuracy += accuracy * word_count\n",
        "        total_words += word_count\n",
        "\n",
        "    accuracy_for_tag = safe_divide(total_accuracy, total_words)\n",
        "\n",
        "    return accuracy_for_tag\n",
        "\n",
        "def calculate_all_accuracies(reference, prediction):\n",
        "    \"\"\"\n",
        "    Calculates overall text extraction accuracy, tag categorization accuracy, and\n",
        "    text extraction accuracy for each tag, including F1 score for tag categorization.\n",
        "    \"\"\"\n",
        "    overall_text_accuracy = calculate_overall_text_extraction_accuracy(\n",
        "        reference, prediction\n",
        "    )\n",
        "    tag_accuracy_dict, overall_tag_accuracy, tag_f1_dict, overall_f1 = calculate_tag_categorization_accuracy(\n",
        "        reference, prediction\n",
        "    )\n",
        "\n",
        "    tag_accuracies = {}\n",
        "    tag_f1_scores = {} # Store F1 scores per tag\n",
        "    for tag in [\n",
        "        \"paragraph\",\n",
        "        \"subheading\",\n",
        "        \"page_footer\",\n",
        "        \"title\",\n",
        "        \"image\",\n",
        "        \"table\",\n",
        "        \"page_header\",\n",
        "        \"code_snippet\",\n",
        "    ]:\n",
        "        tag_accuracies[tag] = calculate_text_accuracy_for_tag(\n",
        "            reference, prediction, tag\n",
        "        )\n",
        "        tag_f1_scores[tag] = tag_f1_dict[tag]\n",
        "\n",
        "    return {\n",
        "        \"overall_text_extraction\": {\"percentage\": overall_text_accuracy},\n",
        "        \"tag_categorization\": {\n",
        "            \"overall_tag_accuracy\": overall_tag_accuracy,\n",
        "            \"tag_accuracy_dict\": tag_accuracy_dict,\n",
        "            \"overall_f1\": overall_f1, # Overall F1 for tag categorization\n",
        "            \"tag_f1_dict\": tag_f1_scores # F1 score per tag\n",
        "        },\n",
        "        \"text_extraction_by_tag\": {\n",
        "            tag: {\"percentage\": tag_accuracies[tag]} for tag in tag_accuracies\n",
        "        },\n",
        "    }\n",
        "\n",
        "def final_accuracy(reference_string, prediction_string):\n",
        "    \"\"\"\n",
        "    Calculates accuracy metrics from tagged input strings, including F1 score.\n",
        "\n",
        "    Args:\n",
        "        reference_string (str): The reference string with tags.\n",
        "        prediction_string (str): The prediction string with tags.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the accuracy and F1 scores, or None if parsing fails.\n",
        "    \"\"\"\n",
        "    reference_data = parse_tagged_string(reference_string)\n",
        "    prediction_data = parse_tagged_string(prediction_string)\n",
        "\n",
        "    if reference_data is None or prediction_data is None:\n",
        "        print(\"Error: Could not parse input strings.\")\n",
        "        return None\n",
        "\n",
        "    return calculate_all_accuracies(reference_data, prediction_data)\n",
        "\n",
        "def avg_accuracy(all_accs):\n",
        "    \"\"\"\n",
        "    Calculates the average accuracy and F1 scores from a list of accuracy dictionaries,\n",
        "    handling missing tags.\n",
        "    \"\"\"\n",
        "    overall_text_accuracies = []\n",
        "    tag_categorization_accuracies = []\n",
        "    tag_accuracy_dicts = []\n",
        "    tag_level_accuracies = defaultdict(list)\n",
        "    tag_f1_dicts = [] # List to store tag F1 dictionaries\n",
        "    overall_f1_scores = [] # List to store overall F1 scores\n",
        "\n",
        "    for acc_dict in all_accs:\n",
        "        overall_text_accuracies.append(acc_dict[\"overall_text_extraction\"][\"percentage\"])\n",
        "        tag_categorization_accuracies.append(\n",
        "            acc_dict[\"tag_categorization\"][\"overall_tag_accuracy\"]\n",
        "        )\n",
        "        tag_accuracy_dicts.append(acc_dict[\"tag_categorization\"][\"tag_accuracy_dict\"])  # Store the dict\n",
        "        tag_f1_dicts.append(acc_dict[\"tag_categorization\"][\"tag_f1_dict\"]) # Store tag F1 dict\n",
        "        overall_f1_scores.append(acc_dict[\"tag_categorization\"][\"overall_f1\"]) # Store overall F1\n",
        "\n",
        "        for tag, metrics in acc_dict[\"text_extraction_by_tag\"].items():\n",
        "            tag_level_accuracies[tag].append(metrics[\"percentage\"])\n",
        "\n",
        "    # Calculate averages, skipping values less than -9998 (-9999)\n",
        "    avg_overall_text_accuracy = np.mean(\n",
        "        [x for x in overall_text_accuracies if x > -9998]\n",
        "    )\n",
        "    avg_tag_categorization_accuracy = np.mean(\n",
        "        [x for x in tag_categorization_accuracies if x > -9998]\n",
        "    )\n",
        "    avg_overall_f1_score = np.mean([x for x in overall_f1_scores if x > -9998]) # Average overall F1\n",
        "\n",
        "    avg_tag_accuracies = {}\n",
        "    avg_tag_f1_scores = {} # Average tag F1 scores\n",
        "    for tag in tag_accuracy_dicts[0]:\n",
        "        avg_tag_accuracies[tag] = np.mean(\n",
        "            [tag_dict[tag] for tag_dict in tag_accuracy_dicts if tag_dict[tag] > -9998]\n",
        "        )\n",
        "        avg_tag_f1_scores[tag] = np.mean(\n",
        "            [tag_dict[tag] for tag_dict in tag_f1_dicts if tag_dict[tag] > -9998 and tag_dict[tag] != -9999] # Handle -9999 for F1\n",
        "        )\n",
        "\n",
        "\n",
        "    avg_tag_level_accuracies = {}\n",
        "    for tag, values in tag_level_accuracies.items():\n",
        "        avg_tag_level_accuracies[tag] = np.mean([x for x in values if x > -9998])\n",
        "\n",
        "    return {\n",
        "        \"overall_text_extraction\": {\"percentage\": avg_overall_text_accuracy},\n",
        "        \"tag_categorization\": {\n",
        "            \"percentage\": avg_tag_categorization_accuracy,\n",
        "            \"tag_accuracy_dict\": avg_tag_accuracies,\n",
        "            \"overall_f1\": avg_overall_f1_score, # Average overall F1\n",
        "            \"tag_f1_dict\": avg_tag_f1_scores # Average tag F1 scores\n",
        "        },\n",
        "        \"text_extraction_by_tag\": {\n",
        "            tag: {\"percentage\": avg_tag_level_accuracies[tag]}\n",
        "            for tag, value in avg_tag_level_accuracies.items()\n",
        "        },\n",
        "    }"
      ],
      "metadata": {
        "id": "hJQ8J_hnvg7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "output_list = []\n",
        "all_accs = []\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "for item in test_data:\n",
        "  processed_item = 'your model tokenized output' #FILL THIS OUT WITH THE SPECIFIED FORMAT ABOVE\n",
        "  processed_data.append({\"prediction\": processed_item, \"target_sequence\": item[\"target_sequence\"]})\n",
        "\n",
        "output_list = []\n",
        "all_accs = []\n",
        "\n",
        "for item in processed_data:\n",
        "  try:\n",
        "    accuracies = final_accuracy(item['target_sequence'], item['prediction'])\n",
        "    # print(accuracies)\n",
        "    if accuracies:\n",
        "        all_accs.append(accuracies)\n",
        "\n",
        "  except Exception as e:  # Catching a broader range of exceptions\n",
        "    print(f\"Skip: {e}\")\n",
        "\n",
        "average_accuracies = avg_accuracy(all_accs)\n",
        "\n",
        "#print all accuracies and F1 scores\n",
        "overall_text_accuracies = average_accuracies['overall_text_extraction']['percentage']\n",
        "tag_categorization_accuracies = average_accuracies['tag_categorization']['percentage']\n",
        "overall_tag_f1 = average_accuracies['tag_categorization']['overall_f1'] # Get overall F1\n",
        "paragraph_text_accuracies = average_accuracies['text_extraction_by_tag']['paragraph']['percentage']\n",
        "image_text_accuracies = average_accuracies['text_extraction_by_tag']['image']['percentage']\n",
        "title_text_accuracies = average_accuracies['text_extraction_by_tag']['title']['percentage']\n",
        "table_text_accuracies = average_accuracies['text_extraction_by_tag']['table']['percentage']\n",
        "page_header_text_accuracies = average_accuracies['text_extraction_by_tag']['page_header']['percentage']\n",
        "subheading_text_accuracies = average_accuracies['text_extraction_by_tag']['subheading']['percentage']\n",
        "code_snippet_text_accuracies = average_accuracies['text_extraction_by_tag']['code_snippet']['percentage']\n",
        "page_footer_text_accuracies = average_accuracies['text_extraction_by_tag']['page_footer']['percentage']\n",
        "\n",
        "# Get tag-specific F1 scores\n",
        "paragraph_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('paragraph', float('nan'))\n",
        "image_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('image', float('nan'))\n",
        "title_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('title', float('nan'))\n",
        "table_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('table', float('nan'))\n",
        "page_header_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('page_header', float('nan'))\n",
        "subheading_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('subheading', float('nan'))\n",
        "code_snippet_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('code_snippet', float('nan'))\n",
        "page_footer_tag_f1 = average_accuracies['tag_categorization']['tag_f1_dict'].get('page_footer', float('nan'))\n",
        "\n",
        "\n",
        "print(f\"Average Overall Text Extraction Accuracy: {overall_text_accuracies:.4f}\")\n",
        "print(f\"Average Tag Categorization Accuracy: {tag_categorization_accuracies:.4f}\")\n",
        "print(f\"Average Overall Tag Categorization F1 Score: {overall_tag_f1:.4f}\") # Print overall F1\n",
        "print(f\"Average Paragraph Extraction Accuracy: {paragraph_text_accuracies:.4f}\")\n",
        "print(f\"Average Image Extraction Accuracy: {image_text_accuracies:.4f}\")\n",
        "print(f\"Average Title Extraction Accuracy: {title_text_accuracies:.4f}\")\n",
        "print(f\"Average Table Extraction Accuracy: {table_text_accuracies:.4f}\")\n",
        "print(f\"Average Page Header Extraction Accuracy: {page_header_text_accuracies:.4f}\")\n",
        "print(f\"Average Subheading Extraction Accuracy: {subheading_text_accuracies:.4f}\")\n",
        "print(f\"Average Code Snippet Extraction Accuracy: {code_snippet_text_accuracies:.4f}\")\n",
        "print(f\"Average Page Footer Extraction Accuracy: {page_footer_text_accuracies:.4f}\")\n",
        "\n",
        "print(\"\\nTag-Specific F1 Scores:\") # Print tag-specific F1 scores\n",
        "print(f\"Paragraph Tag F1 Score: {paragraph_tag_f1:.4f}\")\n",
        "print(f\"Image Tag F1 Score: {image_tag_f1:.4f}\")\n",
        "print(f\"Title Tag F1 Score: {title_tag_f1:.4f}\")\n",
        "print(f\"Table Tag F1 Score: {table_tag_f1:.4f}\")\n",
        "print(f\"Page Header Tag F1 Score: {page_header_tag_f1:.4f}\")\n",
        "print(f\"Subheading Tag F1 Score: {subheading_tag_f1:.4f}\")\n",
        "print(f\"Code Snippet Tag F1 Score: {code_snippet_tag_f1:.4f}\")\n",
        "print(f\"Page Footer Tag F1 Score: {page_footer_tag_f1:.4f}\")"
      ],
      "metadata": {
        "id": "M65CcziCvv5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmUGiWPhJwKv"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3dWjriDXap4"
      },
      "outputs": [],
      "source": [
        "train_images, test_images = train_test_split(image_paths, test_size=.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaKlPh_rSwUM"
      },
      "outputs": [],
      "source": [
        "DOCUMENT_CLASSES = sorted(list(map(lambda p: p.name, Path(\"images\").glob(\"*\"))))\n",
        "DOCUMENT_CLASSES"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = image_paths[300]\n",
        "print(image_path)\n",
        "DOCUMENT_CLASSES.index(image_path.parent.name)"
      ],
      "metadata": {
        "id": "9gsc6BNY_ubh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNzNKgtEUn06"
      },
      "outputs": [],
      "source": [
        "image_path = image_paths[0]\n",
        "print(image_path)\n",
        "DOCUMENT_CLASSES.index(image_path.parent.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w1I29S1JxL5"
      },
      "outputs": [],
      "source": [
        "class DocumentClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, processor):\n",
        "        self.image_paths = image_paths\n",
        "        self.processor = processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        image_path = self.image_paths[item]\n",
        "        json_path = image_path.with_suffix(\".json\")\n",
        "        with json_path.open(\"r\") as f:\n",
        "            ocr_result = json.load(f)\n",
        "\n",
        "            with Image.open(image_path).convert(\"RGB\") as image:\n",
        "\n",
        "                width, height = image.size\n",
        "                width_scale = 1000 / width\n",
        "                height_scale = 1000 / height\n",
        "\n",
        "                words = []\n",
        "                boxes = []\n",
        "                roles = []  # New field for text roles\n",
        "                font_sizes = []\n",
        "                for row in ocr_result:\n",
        "                    boxes.append(scale_bounding_box(row[\"bounding_box\"], width_scale, height_scale))\n",
        "                    words.append(row[\"word\"])\n",
        "                    roles.append(row.get(\"role\", \"body\"))  # Default role is \"body\"\n",
        "                    font_sizes.append(row.get(\"font_size\", 12))\n",
        "\n",
        "                encoding = self.processor(\n",
        "                    image,\n",
        "                    words,\n",
        "                    boxes=boxes,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "        label = DOCUMENT_CLASSES.index(image_path.parent.name)\n",
        "\n",
        "        return dict(\n",
        "            input_ids=encoding[\"input_ids\"].flatten(),\n",
        "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
        "            bbox=encoding[\"bbox\"].flatten(end_dim=1),\n",
        "            pixel_values=encoding[\"pixel_values\"].flatten(end_dim=1),\n",
        "            labels=torch.tensor(label, dtype=torch.long),\n",
        "            roles=roles,  # Pass roles for multitask learning\n",
        "            font_sizes=torch.tensor(font_sizes, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "    def preprocess_with_roles_and_fonts(image, ocr_result, processor, width_scale, height_scale):\n",
        "       words = []\n",
        "       boxes = []\n",
        "       roles = []\n",
        "       font_sizes = []\n",
        "\n",
        "       for row in ocr_result:\n",
        "           boxes.append(scale_bounding_box(row[\"bounding_box\"], width_scale, height_scale))\n",
        "           words.append(row[\"word\"])\n",
        "           roles.append(row.get(\"role\", \"body\"))  # Default role is \"body\"\n",
        "           font_sizes.append(row.get(\"font_size\", 12))  # Default font size\n",
        "\n",
        "           encoding = self.processor(\n",
        "                    image,\n",
        "                    words,\n",
        "                    boxes=boxes,\n",
        "                    max_length=512,\n",
        "                    padding=\"max_length\",\n",
        "                    truncation=True,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "           encoding[\"roles\"] = torch.tensor(roles, dtype=torch.long)\n",
        "           encoding[\"font_sizes\"] = torch.tensor(font_sizes, dtype=torch.float)\n",
        "           return encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6SWardXyXSeg"
      },
      "outputs": [],
      "source": [
        "train_dataset = DocumentClassificationDataset(train_images, processor)\n",
        "test_dataset = DocumentClassificationDataset(test_images, processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-f1O6ZgXmaN"
      },
      "outputs": [],
      "source": [
        "for item in train_dataset:\n",
        "    print(item[\"bbox\"].shape)\n",
        "    print(item[\"pixel_values\"].shape)\n",
        "    print(item[\"labels\"].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O3iuS51YszF"
      },
      "outputs": [],
      "source": [
        "train_data_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "test_data_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=8,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfHnlRP0ct0v"
      },
      "outputs": [],
      "source": [
        "class ModelModule(pl.LightningModule):\n",
        "    def __init__(self, n_classes: int, n_roles: int):\n",
        "        super().__init__()\n",
        "        self.model = LayoutLMv3ForSequenceClassification.from_pretrained(\n",
        "            \"microsoft/layoutlmv3-base\",\n",
        "            num_labels=n_classes\n",
        "        )\n",
        "        self.role_classifier = torch.nn.Linear(self.model.config.hidden_size, n_roles)\n",
        "        self.font_regressor = torch.nn.Linear(self.model.config.hidden_size, 1)\n",
        "        self.train_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
        "        self.val_accuracy = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, bbox, pixel_values, labels=None, roles=None, font_sizes=None):\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            bbox=bbox,\n",
        "            pixel_values=pixel_values,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        role_logits = self.role_classifier(outputs.hidden_states[-1])  # Role classification\n",
        "        font_predictions = self.font_regressor(outputs.hidden_states[-1]).squeeze(-1)  # Font regression\n",
        "        return outputs, role_logits, font_predictions\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        bbox = batch[\"bbox\"]\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        roles = batch[\"roles\"]\n",
        "        font_sizes = batch[\"font_sizes\"]\n",
        "\n",
        "        outputs, role_logits, font_predictions = self(input_ids, attention_mask, bbox, pixel_values, labels, roles, font_sizes)\n",
        "\n",
        "        # Calculate losses\n",
        "        role_loss = torch.nn.CrossEntropyLoss()(role_logits, roles)\n",
        "        font_loss = torch.nn.MSELoss()(font_predictions, font_sizes)\n",
        "        loss = outputs.loss + role_loss + font_loss\n",
        "\n",
        "        # Calculate new accuracy metric\n",
        "        new_accuracy = calculate_text_accuracy_for_tag(labels, outputs.logits)\n",
        "\n",
        "        # Logging loss and accuracy\n",
        "        self.log(\"train_loss\", loss)\n",
        "        self.log(\"train_acc\", self.train_accuracy(outputs.logits, labels), on_step=True, on_epoch=True)\n",
        "        self.log(\"new_train_acc\", new_accuracy, on_step=True, on_epoch=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        bbox = batch[\"bbox\"]\n",
        "        pixel_values = batch[\"pixel_values\"]\n",
        "        labels = batch[\"labels\"]\n",
        "\n",
        "        outputs = self(input_ids, attention_mask, bbox, pixel_values, labels)\n",
        "\n",
        "        val_loss = outputs.loss\n",
        "        val_acc = self.val_accuracy(outputs.logits, labels)\n",
        "        new_val_acc = calculate_text_accuracy_for_tag(labels, outputs.logits)\n",
        "\n",
        "        self.log(\"val_loss\", val_loss, on_epoch=True)\n",
        "        self.log(\"val_acc\", val_acc, on_epoch=True)\n",
        "        self.log(\"new_val_acc\", new_val_acc, on_epoch=True)\n",
        "\n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.00001) #1e-5\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXiLzOk8d0c9"
      },
      "outputs": [],
      "source": [
        "model_module = ModelModule(len(DOCUMENT_CLASSES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_LHFOv8d3rX"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94YKpHEFd6cn"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = ModelCheckpoint(\n",
        "    filename=\"{epoch}-{step}-{val_loss:.4f}\", save_last=True, save_top_k=3, monitor=\"val_loss\", mode=\"min\"\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    accelerator=\"gpu\",\n",
        "    precision=16,\n",
        "    devices=1,\n",
        "    max_epochs=5,\n",
        "    callbacks=[\n",
        "        model_checkpoint\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n-oflwoeA83"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model_module, train_data_loader, test_data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "6A_oJ1kNofAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint.best_model_path"
      ],
      "metadata": {
        "id": "redlOPSvBT3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model = ModelModule.load_from_checkpoint(\n",
        "    model_checkpoint.best_model_path,\n",
        "    n_classes=len(DOCUMENT_CLASSES),\n",
        "    local_files_only=True\n",
        ")\n",
        "\n",
        "# Load test dataset\n",
        "test_results = []\n",
        "for batch in test_data_loader:\n",
        "    outputs = trained_model(batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"bbox\"], batch[\"pixel_values\"])\n",
        "    new_accuracy = calculate_text_accuracy_for_tag(batch[\"labels\"], outputs.logits)\n",
        "    test_results.append(new_accuracy)\n",
        "\n",
        "# Compute final average accuracy\n",
        "average_test_accuracy = sum(test_results) / len(test_results)\n",
        "print(f\"Final Average Test Accuracy: {average_test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "VE2kvBC2r6Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.model.save_pretrained(Path(\"best-model\"))"
      ],
      "metadata": {
        "id": "4PYRvWBJscCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "NDz4i380z0RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model.model.push_to_hub(\"layoutlmv3-unstructured-berkeley-project-1\")"
      ],
      "metadata": {
        "id": "naa03qEP0Ux1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "2LM0f9Tg5bs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LayoutLMv3ForSequenceClassification.from_pretrained(\"layoutlmv3-unstructured-berkeley-project-1\")\n",
        "model = model.eval().to(DEVICE)"
      ],
      "metadata": {
        "id": "zbI1ONios4xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_document_image(\n",
        "    image_path: Path,\n",
        "    model: LayoutLMv3ForSequenceClassification,\n",
        "    processor: LayoutLMv3Processor):\n",
        "\n",
        "    json_path = image_path.with_suffix(\".json\")\n",
        "    with json_path.open(\"r\") as f:\n",
        "        ocr_result = json.load(f)\n",
        "\n",
        "        with Image.open(image_path).convert(\"RGB\") as image:\n",
        "\n",
        "            width, height = image.size\n",
        "            width_scale = 1000 / width\n",
        "            height_scale = 1000 / height\n",
        "\n",
        "            words = []\n",
        "            boxes = []\n",
        "            for row in ocr_result:\n",
        "                boxes.append(\n",
        "                    scale_bounding_box(\n",
        "                        row[\"bounding_box\"],\n",
        "                        width_scale,\n",
        "                        height_scale\n",
        "                    )\n",
        "                )\n",
        "                words.append(row[\"word\"])\n",
        "\n",
        "            encoding = processor(\n",
        "                image,\n",
        "                words,\n",
        "                boxes=boxes,\n",
        "                max_length=512,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output = model(\n",
        "            input_ids=encoding[\"input_ids\"].to(DEVICE),\n",
        "            attention_mask=encoding[\"attention_mask\"].to(DEVICE),\n",
        "            bbox=encoding[\"bbox\"].to(DEVICE),\n",
        "            pixel_values=encoding[\"pixel_values\"].to(DEVICE)\n",
        "        )\n",
        "\n",
        "    predicted_class = output.logits.argmax()\n",
        "    return model.config.id2label[predicted_class.item()]"
      ],
      "metadata": {
        "id": "MYDfQ8y04DJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = []\n",
        "predictions = []\n",
        "for image_path in tqdm(test_images):\n",
        "    labels.append(image_path.parent.name)\n",
        "    predictions.append(predict_document_image(image_path, model, processor))"
      ],
      "metadata": {
        "id": "dqo0sLGu4vSx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
